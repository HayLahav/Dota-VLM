{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOTA-VLM Tutorial\n",
    "## Automated Annotation of Aerial Imagery Using Vision-Language Models\n",
    "\n",
    "This notebook demonstrates how to use the DOTA-VLM pipeline to:\n",
    "1. Detect objects in aerial images\n",
    "2. Crop detected objects\n",
    "3. Generate rich annotations using Vision-Language Models\n",
    "4. Merge and export to COCO format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Add project to path\n",
    "sys.path.append('..')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detection.run_detector import YOLOOBBDetector\n",
    "\n",
    "# Initialize detector\n",
    "detector = YOLOOBBDetector(\n",
    "    model_path='../checkpoints/yolo_obb.pt',\n",
    "    conf_threshold=0.3\n",
    ")\n",
    "\n",
    "# Run detection on a sample image\n",
    "image_path = '../data/DOTA/images/sample.png'\n",
    "detections = detector.detect(image_path)\n",
    "\n",
    "print(f\"Detected {len(detections)} objects\")\n",
    "for det in detections[:5]:  # Show first 5\n",
    "    print(f\"  {det['class_name']}: {det['score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Visualize Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.visualize import visualize_detections\n",
    "\n",
    "# Visualize detections\n",
    "viz_image = visualize_detections(\n",
    "    image_path=image_path,\n",
    "    detections=detections,\n",
    "    draw_labels=True,\n",
    "    color_by_class=True\n",
    ")\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(viz_image[:, :, ::-1])  # BGR to RGB\n",
    "plt.axis('off')\n",
    "plt.title(f'Detections ({len(detections)} objects)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Crop Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.crop_objects import crop_detections\n",
    "\n",
    "# Crop objects\n",
    "crops_metadata = crop_detections(\n",
    "    image_path=image_path,\n",
    "    detections=detections,\n",
    "    output_dir='../crops',\n",
    "    image_id='sample_001',\n",
    "    padding=10\n",
    ")\n",
    "\n",
    "print(f\"Created {len(crops_metadata)} crops\")\n",
    "\n",
    "# Display some crops\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (ax, crop_info) in enumerate(zip(axes, crops_metadata[:10])):\n",
    "    crop_path = Path('../crops') / crop_info['crop_path']\n",
    "    if crop_path.exists():\n",
    "        img = Image.open(crop_path)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"{crop_info['class_name']}\\n{crop_info['score']:.2f}\", fontsize=8)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate VLM Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlm.generate_annotations import LLaVAAnnotator, PromptTemplates\n",
    "\n",
    "# Initialize VLM\n",
    "vlm = LLaVAAnnotator(model_name='llava-hf/llava-1.5-7b-hf')\n",
    "\n",
    "# Annotate first crop as example\n",
    "sample_crop = crops_metadata[0]\n",
    "crop_path = Path('../crops') / sample_crop['crop_path']\n",
    "\n",
    "# Generate attribute description\n",
    "prompt = PromptTemplates.object_attributes()\n",
    "attributes = vlm.generate_caption(str(crop_path), prompt)\n",
    "\n",
    "print(f\"\\nObject: {sample_crop['class_name']}\")\n",
    "print(f\"Detection Score: {sample_crop['score']:.3f}\")\n",
    "print(f\"\\nVLM Attributes:\\n{attributes}\")\n",
    "\n",
    "# Display the crop\n",
    "img = Image.open(crop_path)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img)\n",
    "plt.title(f\"{sample_crop['class_name']} - VLM Annotated\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Batch Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlm.generate_annotations import generate_annotations_for_crop\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Annotate all crops (limited to first 20 for demo)\n",
    "all_annotations = []\n",
    "\n",
    "for crop_info in tqdm(crops_metadata[:20], desc=\"Annotating\"):\n",
    "    crop_path = Path('../crops') / crop_info['crop_path']\n",
    "    \n",
    "    if not crop_path.exists():\n",
    "        continue\n",
    "    \n",
    "    # Generate annotations\n",
    "    annotations = generate_annotations_for_crop(\n",
    "        vlm=vlm,\n",
    "        crop_path=str(crop_path),\n",
    "        crop_metadata=crop_info,\n",
    "        annotation_types=['attributes', 'verification']\n",
    "    )\n",
    "    \n",
    "    all_annotations.append(annotations)\n",
    "\n",
    "print(f\"\\nGenerated {len(all_annotations)} annotations\")\n",
    "\n",
    "# Display some results\n",
    "print(\"\\nSample Annotations:\")\n",
    "print(\"=\"*60)\n",
    "for ann in all_annotations[:3]:\n",
    "    print(f\"\\nClass: {ann['class_name']}\")\n",
    "    print(f\"Attributes: {ann['attributes'][:150]}...\")\n",
    "    print(f\"Verification: {ann['class_verification'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Merge and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.merge_annotations import COCOAnnotationBuilder\n",
    "\n",
    "# Create COCO-style annotations\n",
    "builder = COCOAnnotationBuilder()\n",
    "\n",
    "# Add categories\n",
    "categories = list(set(det['class_name'] for det in detections))\n",
    "builder.add_categories(categories)\n",
    "\n",
    "# Add image\n",
    "image_id = builder.add_image(\n",
    "    image_id=0,\n",
    "    filename='sample.png',\n",
    "    width=1024,\n",
    "    height=1024\n",
    ")\n",
    "\n",
    "# Add annotations\n",
    "for idx, det in enumerate(detections):\n",
    "    # Find corresponding VLM annotation\n",
    "    vlm_metadata = next(\n",
    "        (ann for ann in all_annotations if ann['object_id'] == idx),\n",
    "        None\n",
    "    )\n",
    "    \n",
    "    builder.add_annotation(\n",
    "        ann_id=idx,\n",
    "        image_id=image_id,\n",
    "        category_name=det['class_name'],\n",
    "        bbox=det['bbox'],\n",
    "        detection_score=det['score'],\n",
    "        vlm_metadata=vlm_metadata\n",
    "    )\n",
    "\n",
    "# Build final JSON\n",
    "coco_data = builder.build()\n",
    "\n",
    "# Save\n",
    "output_path = '../outputs/dota_vlm_sample.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(coco_data, f, indent=2)\n",
    "\n",
    "print(f\"✓ Saved DOTA-VLM annotations to {output_path}\")\n",
    "print(f\"  Images: {len(coco_data['images'])}\")\n",
    "print(f\"  Annotations: {len(coco_data['annotations'])}\")\n",
    "print(f\"  Categories: {len(coco_data['categories'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "df_data = []\n",
    "for ann in coco_data['annotations']:\n",
    "    cat_name = next(c['name'] for c in coco_data['categories'] if c['id'] == ann['category_id'])\n",
    "    \n",
    "    row = {\n",
    "        'category': cat_name,\n",
    "        'detection_score': ann['detection_score'],\n",
    "        'has_vlm': 'vlm_metadata' in ann,\n",
    "        'area': ann['area']\n",
    "    }\n",
    "    \n",
    "    if 'vlm_metadata' in ann:\n",
    "        row['attributes'] = ann['vlm_metadata'].get('attributes', '')\n",
    "    \n",
    "    df_data.append(row)\n",
    "\n",
    "df = pd.DataFrame(df_data)\n",
    "\n",
    "# Display statistics\n",
    "print(\"Category Distribution:\")\n",
    "print(df['category'].value_counts())\n",
    "print(\"\\nVLM Coverage:\")\n",
    "print(df['has_vlm'].value_counts())\n",
    "print(\"\\nAverage Detection Score by Category:\")\n",
    "print(df.groupby('category')['detection_score'].mean().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've successfully:\n",
    "1. ✅ Detected objects with oriented bounding boxes\n",
    "2. ✅ Cropped object patches\n",
    "3. ✅ Generated rich VLM annotations\n",
    "4. ✅ Exported to COCO format\n",
    "\n",
    "Next steps:\n",
    "- Scale to full DOTA dataset\n",
    "- Experiment with different VLM models\n",
    "- Fine-tune prompts for better annotations\n",
    "- Train downstream models with enriched annotations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
